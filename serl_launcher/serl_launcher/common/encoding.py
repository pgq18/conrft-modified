from __future__ import annotations

from typing import Dict, Iterable, Optional, Tuple, TYPE_CHECKING

import flax
import flax.linen as nn
import jax
import jax.numpy as jnp
from einops import rearrange, repeat

# Octo imports - only imported when actually needed for OctoEncodingWrapper
try:
    from octo.model.octo_module import OctoTransformer
    from octo.utils.typing import Config, Data, Params, PRNGKey, Sequence
    OCTO_AVAILABLE = True
except ImportError:
    OCTO_AVAILABLE = False
    OctoTransformer = None  # type: ignore

class EncodingWrapper(nn.Module):
    """
    Encodes observations into a single flat encoding, adding additional
    functionality for adding proprioception and stopping the gradient.

    Args:
        encoder: The encoder network.
        use_proprio: Whether to concatenate proprioception (after encoding).
    """

    encoder: nn.Module
    use_proprio: bool
    proprio_latent_dim: int = 64
    enable_stacking: bool = False
    image_keys: Iterable[str] = ("image",)

    @nn.compact
    def __call__(
        self,
        observations: Dict[str, jnp.ndarray],
        train=False,
        stop_gradient=False,
        is_encoded=False,
    ) -> jnp.ndarray:
        # encode images with encoder
        encoded = []
        for image_key in self.image_keys:
            image = observations[image_key]
            if not is_encoded:
                if self.enable_stacking:
                    # Combine stacking and channels into a single dimension
                    if len(image.shape) == 4:
                        T = image.shape[0]
                        if T > 1:
                            image = image[-1:] # for stacked images, only use the last frame
                        image = rearrange(image, "T H W C -> H W (T C)")
                    if len(image.shape) == 5:
                        T = image.shape[1]
                        if T > 1:
                            image = image[:, -1:] # for stacked images, only use the last frame
                        image = rearrange(image, "B T H W C -> B H W (T C)")

            image = self.encoder[image_key](image, train=train, encode=not is_encoded)

            if stop_gradient:
                image = jax.lax.stop_gradient(image)

            encoded.append(image)

        encoded = jnp.concatenate(encoded, axis=-1)

        if self.use_proprio:
            # project state to embeddings as well
            state = observations["state"]
            if self.enable_stacking:
                # Combine stacking and channels into a single dimension
                if len(state.shape) == 2:
                    state = rearrange(state, "T C -> (T C)")
                    encoded = encoded.reshape(-1)
                if len(state.shape) == 3:
                    state = rearrange(state, "B T C -> B (T C)")
            state = nn.Dense(
                self.proprio_latent_dim, kernel_init=nn.initializers.xavier_uniform()
            )(state)
            state = nn.LayerNorm()(state)
            state = nn.tanh(state)
            encoded = jnp.concatenate([encoded, state], axis=-1)

        return encoded
    
class OctoEncodingWrapper(nn.Module):
    """
    Encodes observations into a single flat encoding, adding additional
    functionality for adding proprioception and stopping the gradient.

    Args:
        encoder: The encoder network.
        use_proprio: Whether to concatenate proprioception (after encoding).
    """

    encoder: OctoTransformer
    use_proprio: bool
    proprio_latent_dim: int = 64
    enable_stacking: bool = False
    image_keys: Iterable[str] = ("image",)

    def __setup__(self):
        if not OCTO_AVAILABLE:
            raise ImportError(
                "OctoEncodingWrapper requires octo to be installed. "
                "Please install octo or use EncodingWrapper with resnet encoder instead. "
                "To install octo, follow the instructions in CLAUDE.md."
            )

    @nn.compact
    def __call__(
        self,
        observations: Dict[str, jnp.ndarray],
        tasks: Data=None,
        action_embeddings: jnp.ndarray=None,
        train=True,
        stop_gradient=False,
    ) -> jnp.ndarray:
        if action_embeddings is None:
            # Use image_keys from configuration instead of hardcoded values
            image_primary_key = self.image_keys[0]
            image_wrist_key = self.image_keys[1] if len(self.image_keys) > 1 else self.image_keys[0]

            image_primary = observations[image_primary_key]
            image_wrist = observations[image_wrist_key]
            if image_primary.ndim == 4:
                image_primary = image_primary[jnp.newaxis, ...]
                image_wrist = image_wrist[jnp.newaxis, ...]

            # Resize images to match Octo's expected sizes
            # Octo expects: image_primary at 256x256, image_wrist at 128x128
            if image_primary.shape[-2] != 256 or image_primary.shape[-3] != 256:
                # Resize image_primary to 256x256
                image_primary = jax.image.resize(image_primary, shape=image_primary.shape[:-3] + (256, 256) + image_primary.shape[-1:],
                                                method='bilinear')
            if image_wrist.shape[-2] != 128 or image_wrist.shape[-3] != 128:
                # Resize image_wrist to 128x128
                image_wrist = jax.image.resize(image_wrist, shape=image_wrist.shape[:-3] + (128, 128) + image_wrist.shape[-1:],
                                              method='bilinear')

            batch_size = image_primary.shape[0]
            window_size = image_primary.shape[1]
            timestep_pad_mask = jnp.ones((batch_size, window_size), dtype=bool)
            
            if not stop_gradient:
                def mask_image(image, mask_flag):
                    return jax.lax.cond(
                        mask_flag,
                        lambda _: jnp.zeros_like(image),
                        lambda _: image, 
                        operand=None)
                
                mask_flags = jax.random.bernoulli(self.make_rng('mask_wrist'), p=0.2, shape=(batch_size,))
                image_wrist = jax.vmap(mask_image)(image_wrist, mask_flags)
            
            observation_octo = {"image_primary": image_primary,
                "image_wrist": image_wrist, 
                "timestep_pad_mask": timestep_pad_mask,
            }

            transformer_outputs = self.encoder(observation_octo, tasks, timestep_pad_mask, train=not stop_gradient)
            token_group = transformer_outputs["readout_action"]
            action_embeddings = token_group.tokens.mean(axis=-2)
            
            action_embeddings = action_embeddings[:, -1, :] # remove window_size dimension
        else:
            action_embeddings = action_embeddings
            
        if stop_gradient:
            action_embeddings = jax.lax.stop_gradient(action_embeddings)

        encoded = action_embeddings
        if self.use_proprio:
            # project state to embeddings as well
            state = observations["state"]
            if self.enable_stacking:
                # Combine stacking and channels into a single dimension
                if len(state.shape) == 2:
                    state = rearrange(state, "T C -> (T C)")
                    encoded = encoded.reshape(-1)
                if len(state.shape) == 3:
                    state = rearrange(state, "B T C -> B (T C)")
            state = nn.Dense(
                self.proprio_latent_dim, kernel_init=nn.initializers.xavier_uniform()
            )(state)
            state = nn.LayerNorm()(state)
            state = nn.tanh(state)
            encoded = jnp.concatenate([encoded, state], axis=-1)

        return encoded, action_embeddings


